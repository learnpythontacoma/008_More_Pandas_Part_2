{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='graphics/panda1-getty.jpg'>\n",
    "\n",
    "# Pandas, Part 2\n",
    "\n",
    "In our last meet-up, we introduced the Pandas library and began to work with some basic exploratory tools:\n",
    "1. Importing the Pandas library\n",
    "1. Creating DataFrames\n",
    "    1. Creating an Empty DataFrame\n",
    "    1. Creating a DataFrame from a Python list\n",
    "    1. Importing a CSV as a Pandas DataFrame (one of the most common ways to import data into Python)\n",
    "1. .head( )\n",
    "1. .tail( )\n",
    "1. .sample( )\n",
    "1. .shape\n",
    "1. .keys( ) & .columns\n",
    "1. .info( )\n",
    "1. .describe( )\n",
    "1. .values_count( )\n",
    "1. Accessing a feature (column) in a DataFrame\n",
    "1. Accessing an observation (row) in a DataFrame\n",
    "1. Navigating a DataFrame using .loc( ) and .iloc( )\n",
    "\n",
    "These commands helped us to begin establishing or load a DataFrame, then begin exploring the new DataFrame's information. To warm up, let's load and do a preliminary examination of a dataset. \n",
    "\n",
    "# Warm-up Practice\n",
    "\n",
    "In the `data` folder, you will find a .csv file called, `rock.csv`. In the next blocks, \n",
    "1. Load the `rock.csv` file as a DataFrame and assign it to the variable `rock_df`\n",
    "1. Determine the size of the DataFrame (number of observations, number of features)\n",
    "1. Print out the names of the features (columns)\n",
    "1. Print out the first five observations (rows)\n",
    "1. Print out the last five observations (rows)\n",
    "1. Determine the feature type and number of non-null values for each feature\n",
    "1. Determine the DataFrame file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rock.csv file as a DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the size of the DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the names of the features (columns) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first five observations (rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the last five observations (rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the feature type and number of non-null values for each feature\n",
    "\n",
    "\n",
    "\n",
    "# Determine the DataFrame file size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GREAT JOB!**\n",
    "\n",
    "We loaded a DataFrame and we began to get an idea of the data inside of it. \n",
    "\n",
    "A phrase that you will hear often, say often after hearing it, and learn its truth by experience is \n",
    "    90% of a data analyst's or data scientist's time is spent cleaning data\n",
    "    \n",
    "As we look at the dataset we just loaded, we see a couple of things that bring truth to this statement. \n",
    "1. **What do you notice about the feature names?**\n",
    "1. **What do you notice about the number of non-null values in the dataset?**\n",
    "\n",
    "Let's do a bit of cleaning, starting with the feature (column) names. \n",
    "\n",
    "\n",
    "# Cleaning up column names\n",
    "\n",
    "There are three ways to clean up column names. \n",
    "\n",
    "## Method 1 - Clean when you import the dataset\n",
    "\n",
    "The first method is to assign new names to the column as the DataFrame is imported through the `pd.read_csv()` command.To clean column names as a DataFrame is being read into Python, you pass a list of strings to Python. The list of strings has as many strings passed as there are columns in the DataFrame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names when loading the '.csv':\n",
    "column_names = ['song', 'artist', 'release', 'song_artist', 'first', 'year', 'playcount', 'fg'] #Pass a list \n",
    "rock_df2 = pd.read_csv('data/rock.csv', names=column_names, skiprows=1) #establish a new DF\n",
    "rock_df2.head() #Look at the DF head to see the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**NOTE**: When you create custom column names, the first row of the `.csv` already represents a header. It is important to tell `pandas` to skip that row. The `skiprows=1` keyword argument to `read_csv()` will tell `pandas` to skip the first row.\n",
    "___\n",
    "\n",
    "\n",
    "\n",
    "## Method 2 - .rename( )\n",
    "\n",
    "The `.rename()` function takes an argument, `columns=name_dict`, in which `name_dict` is a dictionary containing the original column names as keys and the new column names as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names using the `.rename()` function.\n",
    "\n",
    "rock_df3 = pd.read_csv('data/rock.csv')\n",
    "\n",
    "rename_map = {\n",
    "    # Original column: [renamed column]\n",
    "    'Song Clean':    'song', \n",
    "    'ARTIST CLEAN':  'artist', \n",
    "    'Release Year':  'release', \n",
    "    'COMBINED':      'song_artist', \n",
    "    'First?':        'first', \n",
    "    'Year?':         'year', \n",
    "    'PlayCount':     'playcount', \n",
    "    'F*G':           'fg'\n",
    "}\n",
    "\n",
    "rock_df3.rename(columns=rename_map, inplace=True)#The inplace value tells python to run the command.\n",
    "rock_df3.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 - Reassigning the `.columns` attribute of a DataFrame\n",
    "\n",
    "You can also just reassign the `.columns` attribute to a list of strings containing the new column names. \n",
    "\n",
    "The only caveat with reassigning `.columns` is that you have to reassign all of the column names at once. You can't partially replace a value by working on `.columns` directly. You have to reassign `.columns` with a list of equal length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the column names by reassigning the `.columns` attribute.\n",
    "rock_df4 = pd.read_csv('data/rock.csv')\n",
    "column_names = ['song', 'artist', 'release', 'song_artist', 'first', 'year', 'playcount', 'fg']\n",
    "rock_df4.columns = column_names\n",
    "\n",
    "rock_df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the NaN values\n",
    "\n",
    "### .isnull( ) \n",
    "\n",
    "As we discovered above, there are several NaN values in the \"release\" column. So let's clean these to zeros. \n",
    "\n",
    "We have mixed `str` and `NaN` values in the `release` column. `NaN` stands for \"not a number\". This is the way Pandas handles \"nulls\" or nonexistent data. We can use the `.isnull()` method of a Series to find null values.\n",
    "\n",
    "To start, let's get a subset of the DataFrame where only the observations (rows) with NaN values in the \"release\" column are showing. To do this, we will use a \"mask\". \n",
    "\n",
    "First, we will establish a variable that pulls the coordinates for the observations we want to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show us the records where there is a 'null' value in the \"release\" column\n",
    "\n",
    "null_release_values = rock_df4['release'].isnull() \n",
    "#null_release_values is only a variable name. You can use any that you want for the variable. \n",
    "\n",
    "rock_df4[null_release_values].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update slices of your DataFrame based on mask selection/slices.\n",
    "\n",
    "In many scenarios, we want to upate values in our DataFrame according to criteria. Let's say we wanted to set all of the null values in `release` to 0.\n",
    "\n",
    "With newer versions of `pandas`, in order to manipulate data in the original DataFrame, we have to use `.loc` while performing reassignment using a mask and an index.\n",
    "\n",
    "For example, the following won't always work:\n",
    "```python\n",
    "    df[row_mask]['column_name'] = new_value\n",
    "```\n",
    "\n",
    "The best way to accomplish the same task is:\n",
    "```python\n",
    "    df.loc[row_mask, 'column_name'] = new_value\n",
    "```\n",
    "\n",
    "For multiple column assignment, you would use:\n",
    "```python\n",
    "    df.loc[row_mask, ['col_1', 'col_2', 'col_3']] = new_value\n",
    "```\n",
    "\n",
    "Let's try it out. Make all of the null values in `release` 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to reload our data to a fresh state.\n",
    "column_names = ['song', 'artist', 'release', 'song_artist', 'first', 'year', 'playcount', 'fg']\n",
    "df = pd.read_csv('data/rock.csv', names=column_names, skiprows=1)# we changed the name of the df for this\n",
    "\n",
    "# We are creating/identifying the target subset and using it as an indexing tool to set values\n",
    "null_release_value = df['release'].isnull()\n",
    "\n",
    "# We are changing the NaN values in \"release\" to zero\n",
    "df.loc[null_release_value, 'release'] = 0\n",
    "\n",
    "# We'll then print out our DataFrame's first 15 rows to verify\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the `.sum()` command - a new command - to verify that \"release\" contains no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure that the data types of the columns make sense. \n",
    "\n",
    "Verifying column data types is a critical part of data munging. If columns have the wrong data type, then there is usually corrupted or incorrect data in some of the observations.\n",
    "\n",
    "#### Look at the data types for the columns. Are any incorrect given what the data represents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Only the `release` column appears to be wrong. It is represented as a string but should be an integer for year._\n",
    "\n",
    "### Investigate and clean up the `release` column.\n",
    "\n",
    "The `release` column is a string data type when it should be an integer.\n",
    "\n",
    "#### Figure out what value(s) are causing the `release` column to be encoded as a string instead of an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the unique values in the column can be a good way to find offending values:\n",
    "df.release.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A row has SONGFACTS.COM as a value â€” this is clearly not a year.\n",
    "\n",
    "#### Look at the rows in which there is incorrect data in the `release` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice and assign\n",
    "release_mask = (df['release'] == \"SONGFACTS.COM\")\n",
    "\n",
    "# Show the offending observation (row)\n",
    "df[release_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the data. \n",
    "\n",
    "We previously converted all of the nan values in the release column to zeros so we might as well continue with the same practice (there are other ways which we will discuss in the future). Replacing with zero (or nan) will allow us to convert the column to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign a value of zero to the offending row\n",
    "df.loc[release_mask, 'release'] = 0\n",
    "\n",
    "# Change the \"release\" column to integers\n",
    "df['release'] = df['release'].astype(float)\n",
    "\n",
    "# Verify that the change was made\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the unique values for the `release` column. Do the release dates look correct?\n",
    "\n",
    "From what we learned above, print out the unique values of the \"release\" column, so we can examine them to insure they are now correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.release.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "Given everything we have learned so far in this lesson and from the last meet-up,\n",
    "1. Find the observation (row) that has the '1071' error in it\n",
    "1. Correct the error\n",
    "1. Verify that the error is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the observation (row) that has the error in the release column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the error - the correct year is 1971\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the error is corrected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
